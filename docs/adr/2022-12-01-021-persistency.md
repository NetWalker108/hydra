---
slug: 21
title: |
  21. Persistency
authors: []
tags: [Proposed]
---

## Status

Proposed

## Context

As any other process, a Hydra node can crash. If a head is open when it crashes then it's important that, when restarting the node, it can get to a meaningfull state regarding this head:
* who are the peers in the head?
* what was the last validated snapshot?
* what are the seen transaction?
* what is the status of the head (still open, closed, aborted)?

It's also important for an API client to be able to restore its own view of the world when re-connecting to a Hydra node after a restart.

All these persistent mechanisms should be reliable and have a limited performance impact on the system.

We've already implemented a form of state persistency in [#187](https://github.com/input-output-hk/hydra/issues/187)
and then brought some fixes in [#599](https://github.com/input-output-hk/hydra/issues/599)

We then have implemented a solution for an API client to recover its own state in [#580](https://github.com/input-output-hk/hydra/issues/580).

### Current solution

Currently, persistency is achieved by storing data in two files on disk:
* state
* server-output

_state_ would store the state of the process as a single json object and _server-output_ would store each output sent to an API client to replay it later if need be.

#### Persisting the state

The _state_ file stores the state of a hydra node as a JSON object. The state on disk would be updated everytime the state is updated:

```haskell
stepHydraNode tracer node = do
  -- ...
  outcome <- atomically (processNextEvent node e)
  case outcome of
    -- ...
    NewState s effs -> do
      save s
      -- ...
```

In principle, everytime we save the state, we overwrite the data stored in the file named _state_. To prevent all sort of issues with updating data on disk we use [writeBinaryFileDurableAtomic](https://hackage.haskell.org/package/unliftio-0.2.23.0/docs/UnliftIO-IO-File.html#v:writeBinaryFileDurableAtomic). So every state update on disk implies one copy of the original file, then an update of the content of the file with the full data of the new state, then an on disk _atomic_ move of the file to the original _state_ place:

![](img/021-persisting_the_state.jpg)

#### Persisting server output

For a client connecting through the API to reconstruct its state, we simply send to it all the history of events that happened before.
So that we can do that after a hydra node restarts, we need some form of persistence.

Today, we implement it by simply storing on disk every message we send to a client. On restart, we would read that again from disk and send all these messages again to any newly connected client.

The `server-output` file stores each of this messages in an append only file as a list of JSON object. Objects are separated by a newline character.
This is alos when we store them on disk that we give an id and a timestamp to each of these outputs:

```haskell
  appendToHistory history output = do
    time <- getCurrentTime
    timedOutput <- atomically $ do
      seq <- nextSequenceNumber <$> readTVar history
      let timedOutput = TimedServerOutput{output, time, seq}
      modifyTVar' history (timedOutput :)
      pure timedOutput
    append timedOutput
    pure timedOutput
```

The _append_ function uses [withBinaryFileDurable](https://hackage.haskell.org/package/unliftio-0.2.23.0/docs/UnliftIO-IO-File.html#v:withBinaryFileDurable)
 to append data on disk. Some reliability is ensured since we only append data to the file and never change existing data.
We may have problem if the last line of the file contains a partially writtent JSON object.

Also note that, with the current implementation, we open and close the file for every server-output we want to persist.

#### Drawbacks

The current solution has several drawbacks. We describe some of them here.

Extra disk usage with the state persistency:
* copy of the file before overriding everything;
* Serialization of the whole state everytime.

The state is maintained in two separate files, one for server-output and one for state which may lead to inconsistencies in some corner case situations.

When a customer send a _getUTxO_ query, for instance, the answer to this query is stored in the _server-out_ file and will then be replayed after a restart
eventhough it wouldn't make much sense as the newly connected client didn't make any request. It could be helpful do differentiate between request answers
and actual events which should be persisted.

When a new peer is seen, a message is sent to the API client and this is stored in _server-out_. So on restart, the message will be replayed after a restart
and the client will be informed that the peer is connected eventhough it is certainly not yet connected, just after a restart. As a counter measure, we
also send a _Greetings_ message, later, to notify the client that the hydra node has been restarted and that it should _forget_ about some information
received before. But may be we could have a persistent that could distinguish volatile events from persistent ones?


### event/command sourcing as an alternative

[Event sourcing](https://martinfowler.com/eaaDev/EventSourcing.html) could be an alternative to store the state of the Hydra node. In that kind of architecture,
everytime something happens that could update the state, an event is generated, then stored on disk, then applied to the state.

Some of the values could be:
* More effecient perisitence of the state (only events are happended on disk, not the full state and no need for file copying);
* reliability as the state is only updated after the events have been persisted on disk.

That may also allow us to improve client API replay of events by introducing some logic at the server API level to decide what is worth replaying and what is not.

It may be the case that it could also help us with hydra peers network communication issues: a peer could ask for the replay of a list of events from another peer?

It might be the case that not all the inputs of our system could be mapped to the concept of _command_ from event-sourcing architecture. In particulier changes
observed on-chain. Here is a diagram of a possible _event sourcing_ inspired architecture for a Hydra node:

![](img/021-event-sourcing.png)

In the above diagram, all the outputs would have to be stored on disk before being propagated and integrated to the state.

An alternative would be to go with a command sourcing approach in which, instead of storing the events produced by our system
(that is the outputs of the head logic in the above diagram), we would store the commands received by the system (that is the
inputs of the head logic in the above diagram).

As of this writing, it is not clear if it would be superior or more problematic to command sourcing instead of event sourcing.

## Decision

We implement an event sourcing architecture.

## Consequences

To be defined.

